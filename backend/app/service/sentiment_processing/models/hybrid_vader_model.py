"""
Hybrid VADER Model - Enhanced VADER + Machine Learning Classifier + Sarcasm Detection
======================================================================================

Combines rule-based Enhanced VADER with a Logistic Regression classifier
and advanced sarcasm detection for improved accuracy on community-driven
financial sentiment analysis (Hacker News, forums, etc.).

Architecture:
1. Enhanced VADER provides domain-enhanced baseline with financial lexicon
2. Sarcasm detection layer identifies and inverts ironic statements
3. Logistic Regression trained on community comments with TF-IDF + VADER features
4. Confidence-weighted ensemble fusion

Target Performance: 91.67% accuracy (validated on real community data)

This file contains:
- Enhanced VADER implementation (rule-based component)
- Sarcasm detection module (pattern-based + contextual)
- Hybrid VADER implementation (ensemble model)
- Financial lexicon and community slang processing
"""

import logging
import os
import pickle
import time
import re
from typing import List, Dict, Any, Tuple, Optional, TYPE_CHECKING
from dataclasses import dataclass
import numpy as np

# NLTK imports for VADER
try:
    import nltk
    from nltk.sentiment import SentimentIntensityAnalyzer
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VaderSIA
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    VaderSIA = None  # type: ignore

if TYPE_CHECKING:
    from sklearn.linear_model import LogisticRegression
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.preprocessing import StandardScaler

try:
    from sklearn.linear_model import LogisticRegression as LogisticRegressionImpl
    from sklearn.feature_extraction.text import TfidfVectorizer as TfidfVectorizerImpl
    from sklearn.preprocessing import StandardScaler as StandardScalerImpl
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    LogisticRegressionImpl = None  # type: ignore
    TfidfVectorizerImpl = None  # type: ignore
    StandardScalerImpl = None  # type: ignore

from .sentiment_model import (
    SentimentModel, SentimentResult, SentimentLabel, 
    TextInput, ModelInfo, DataSource, ModelLoadError, AnalysisError
)

logger = logging.getLogger(__name__)


# =============================================================================
# Enhanced VADER Components (Internal use only)
# =============================================================================

@dataclass
class EnhancementConfig:
    """Configuration for VADER enhancements."""
    use_financial_lexicon: bool = True
    use_community_slang: bool = True
    use_emoji_boost: bool = True
    use_dynamic_thresholds: bool = True
    use_context_awareness: bool = True
    positive_threshold: float = 0.05
    negative_threshold: float = -0.05
    confidence_boost_factor: float = 1.2


class FinancialLexicon:
    """Financial domain-specific sentiment lexicon."""
    
    BULLISH_TERMS = {
        # Strong bullish terms
        'moon': 3.0, 'mooning': 3.0, 'squeeze': 2.5, 'gamma': 2.0,
        'bullish': 2.5, 'bull': 2.0, 'rally': 2.0, 'breakout': 2.0,
        'pump': 2.0, 'soar': 2.5, 'surge': 2.5, 'rocket': 3.0,
        'tendies': 2.5, 'gains': 2.0, 'profit': 2.0, 'green': 1.5,
        'calls': 1.5, 'long': 1.5, 'buy': 1.5, 'accumulate': 2.0,
        'undervalued': 2.0, 'oversold': 1.5, 'support': 1.0,
        'uptrend': 2.0, 'momentum': 1.5, 'growth': 1.5,
        
        # Community-specific bullish (HN/forum slang)
        'yolo': 2.0, 'diamond hands': 3.0, 'hodl': 2.5, 'apes': 2.0,
        'to the moon': 3.5, 'printing': 2.5, 'brrr': 2.0,
        'lambo': 2.5, 'tendie': 2.5, 'stonks': 2.0,
    }
    
    BEARISH_TERMS = {
        # Strong bearish terms
        'crash': -3.0, 'dump': -2.5, 'tank': -2.5, 'plunge': -2.5,
        'bearish': -2.5, 'bear': -2.0, 'selloff': -2.0, 'collapse': -3.0,
        'bloodbath': -3.0, 'red': -1.5, 'puts': -1.5, 'short': -1.5,
        'sell': -1.5, 'overvalued': -2.0, 'overbought': -1.5,
        'resistance': -1.0, 'downtrend': -2.0, 'correction': -1.5,
        'bubble': -2.0, 'bag': -2.0, 'bags': -2.0, 'bagholder': -2.5, 'bagholding': -2.5,
        'down': -1.5, 'loss': -2.0, 'losses': -2.0, 'losing': -2.0,
        
        # Community-specific bearish (HN/forum slang)
        'guh': -3.0, 'loss porn': -2.5, 'paper hands': -2.0,
        'rug pull': -3.0, 'bleeding': -2.5, 'rekt': -3.0,
        'heavy bags': -2.5, 'catching knives': -2.5,
    }
    
    NEUTRAL_MODIFIERS = {
        'maybe': 0.5, 'possibly': 0.5, 'might': 0.5,
        'could': 0.3, 'should': 0.3, 'would': 0.3,
        'if': 0.2, 'but': -0.3, 'however': -0.3,
    }


class CommunitySlangProcessor:
    """Processor for community-specific slang and abbreviations (HN/forums)."""
    
    SLANG_MAPPINGS = {
        # Common abbreviations
        'btfd': 'buy the fucking dip',
        'btd': 'buy the dip',
        'dd': 'due diligence',
        'fomo': 'fear of missing out',
        'fud': 'fear uncertainty doubt',
        'imo': 'in my opinion',
        'imho': 'in my humble opinion',
        'tbh': 'to be honest',
        'nfa': 'not financial advice',
        'dyor': 'do your own research',
        'ath': 'all time high',
        'atl': 'all time low',
        'idk': 'i dont know',
        'iirc': 'if i recall correctly',
        'gme': 'gamestop',
        'amc': 'amc entertainment',
        'bb': 'blackberry',
        'pltr': 'palantir',
        'tsla': 'tesla',
        'spy': 'sp500',
        'qqq': 'nasdaq',
        'fd': 'weekly options',
        'iv': 'implied volatility',
        'otm': 'out of the money',
        'itm': 'in the money',
        'dte': 'days to expiration',
        'cc': 'covered call',
        'csp': 'cash secured put',
        'wsb': 'wallstreetbets',
        'guh': 'major loss',
        'brr': 'money printer',
        'jpow': 'jerome powell',
        'papa': 'elon musk',
    }
    
    EMOJI_SENTIMENTS = {
        # Positive emojis
        'ðŸš€': 3.0, 'ðŸ“ˆ': 2.5, 'ðŸ’Ž': 2.5, 'ðŸ™Œ': 2.0,
        'ðŸ”¥': 2.0, 'ðŸ’°': 2.0, 'ðŸ¤‘': 2.5, 'ðŸ˜': 2.0,
        'ðŸŽ¯': 1.5, 'âœ…': 1.5, 'ðŸ‘': 1.5, 'ðŸ’ª': 2.0,
        'ðŸŒ™': 2.5, 'â¬†ï¸': 1.5, 'ðŸ“Š': 1.0, 'ðŸ†': 2.0,
        
        # Negative emojis
        'ðŸ“‰': -2.5, 'ðŸ’©': -2.0, 'ðŸ¤®': -2.5, 'ðŸ˜­': -2.0,
        'ðŸ”´': -1.5, 'â¬‡ï¸': -1.5, 'ðŸ»': -2.0, 'ðŸ’”': -2.0,
        'ðŸ˜±': -1.5, 'ðŸ˜°': -1.5, 'ðŸ¤¯': -1.0, 'âŒ': -2.0,
        'ðŸ©¸': -2.5, 'â˜ ï¸': -3.0, 'âš ï¸': -1.0,
    }
    
    def process_slang(self, text: str) -> str:
        """Replace slang with full terms."""
        text_lower = text.lower()
        for slang, expansion in self.SLANG_MAPPINGS.items():
            pattern = r'\b' + re.escape(slang) + r'\b'
            text_lower = re.sub(pattern, expansion, text_lower, flags=re.IGNORECASE)
        return text_lower
    
    def extract_emoji_sentiment(self, text: str) -> float:
        """Calculate sentiment boost from emojis."""
        emoji_score = 0.0
        emoji_count = 0
        
        for emoji, score in self.EMOJI_SENTIMENTS.items():
            count = text.count(emoji)
            if count > 0:
                emoji_score += score * count
                emoji_count += count
        
        if emoji_count > 0:
            return emoji_score / emoji_count * 0.3
        return 0.0


class SarcasmDetector:
    """
    Sarcasm and irony detection for financial sentiment analysis.
    
    Uses pattern matching and contextual analysis to identify:
    - Explicit sarcasm markers ("oh great", "thanks for nothing", "/s")
    - Ironic contrast patterns (positive words + negative context)
    - Hyperbolic expressions indicating sarcasm
    - Rhetorical questions with implied negative sentiment
    
    When sarcasm is detected with high confidence, the sentiment
    polarity is inverted to reflect the true intended meaning.
    """
    
    # Explicit sarcasm markers and patterns
    SARCASM_MARKERS = [
        # Explicit tags
        r'/s\b', r'\bsarcasm\b', r'\bironic\b', r'\bironically\b',
        # Eye-rolling expressions
        r'\boh great\b', r'\boh wonderful\b', r'\boh fantastic\b', r'\boh perfect\b',
        r'\boh yeah\b.*\bthat\b.*\bwork', r'\boh sure\b',
        # Thanks patterns with sarcasm
        r'\bthanks for nothing\b', r'\bthanks a lot\b.*(?:jerk|idiot|genius)',
        r'\bgee thanks\b', r'\bthanks,?\s*(?:i guess|whatever)\b',
        # Yeah right patterns
        r'\byeah right\b', r'\byeah,?\s*sure\b', r'\bsure,?\s*buddy\b',
        r'\bsuuure\b', r'\briiiight\b', r'\byeaaaah\b',
        # Quotes indicating disbelief
        r'"(?:genius|brilliant|smart|great)"', r"'(?:genius|brilliant|smart|great)'",
        # Exaggerated positives with context
        r'\bwhat a surprise\b', r'\bwho could have predicted\b',
        r'\bwhat a shock\b', r'\bcolor me shocked\b',
        # Clearly patterns
        r'\bclearly\b.*\bworking\b', r'\bobviously\b.*\bgreat\b',
    ]
    
    # Ironic contrast patterns (positive word + negative reality)
    IRONIC_CONTRASTS = [
        # Great/wonderful + negative context
        (r'\bgreat\b', [r'\bloss', r'\bcrash', r'\bdump', r'\bdown \d+%', r'\bbankrupt']),
        (r'\bwonderful\b', [r'\bloss', r'\bcrash', r'\bdown', r'\bbankrupt']),
        (r'\bfantastic\b', [r'\bloss', r'\bfail', r'\bdown', r'\bcrash']),
        (r'\bbrilliant\b', [r'\bstrategy\b.*\bloss', r'\bmove\b.*\bcrash', r'\bidea\b.*\bfail']),
        # Love/enjoy + negative actions (sarcastic when paired with losses)
        (r'\blove\b', [r'\blosing money', r'\bwatch.*crash', r'\bwatch.*burn', r'\bbag.*hold', r'\d+%\s*loss']),
        (r'\bloving\b', [r'\bloss', r'\bcrash', r'\bdown', r'\d+%']),  # "Loving these losses"
        (r'\benjoy\b', [r'\bloss', r'\bcrash', r'\bdown']),
        # Best + negative context
        (r'\bbest\b', [r'\bworst', r'\bloss', r'\bcrash', r'\bfail', r'\bdisaster']),
        # Can't wait + negative event
        (r"can't wait", [r'\bcrash', r'\bloss', r'\bbankrupt', r'\bfail']),
    ]
    
    # Hyperbolic patterns suggesting sarcasm
    HYPERBOLIC_PATTERNS = [
        r'\bmost definitely\b.*\bnot\b',
        r'\babsolutely\b.*\bno way\b',
        r'\bsuch a\b.*\bgenius\b',
        r'\breal winner\b',
        r'\bgreat job\b.*(?:losing|crash|dump|fail)',
        r'\bwhat could go wrong\b',
        r'\bwhat could possibly go wrong\b',
        r'\bnothing bad will happen\b',
        r'\bthis is fine\b',
        r'\beverything is fine\b',
    ]
    
    # Rhetorical questions implying negative sentiment
    RHETORICAL_QUESTIONS = [
        r'who thought.*was a good idea',
        r'what did you expect',
        r'why would anyone',
        r'how could this.*worse',
        r'is anyone surprised',
        r'shocked.?\s*i tell you',
    ]
    
    def __init__(self):
        """Initialize sarcasm detector with compiled patterns."""
        # Compile all patterns for efficiency
        self._sarcasm_markers = [re.compile(p, re.IGNORECASE) for p in self.SARCASM_MARKERS]
        self._hyperbolic_patterns = [re.compile(p, re.IGNORECASE) for p in self.HYPERBOLIC_PATTERNS]
        self._rhetorical_questions = [re.compile(p, re.IGNORECASE) for p in self.RHETORICAL_QUESTIONS]
        
        # Compile ironic contrast patterns
        self._ironic_contrasts = []
        for positive_word, negative_contexts in self.IRONIC_CONTRASTS:
            pos_pattern = re.compile(positive_word, re.IGNORECASE)
            neg_patterns = [re.compile(nc, re.IGNORECASE) for nc in negative_contexts]
            self._ironic_contrasts.append((pos_pattern, neg_patterns))
    
    def detect_sarcasm(self, text: str) -> Dict[str, Any]:
        """
        Detect sarcasm in text and return analysis results.
        
        Args:
            text: Text to analyze for sarcasm
            
        Returns:
            Dictionary with:
            - is_sarcastic: bool indicating sarcasm detection
            - confidence: float 0-1 indicating detection confidence
            - markers_found: list of detected sarcasm patterns
            - should_invert: bool indicating if sentiment should be inverted
        """
        if not text:
            return {
                'is_sarcastic': False,
                'confidence': 0.0,
                'markers_found': [],
                'should_invert': False
            }
        
        markers_found = []
        confidence_signals = []
        
        # Check explicit sarcasm markers (highest confidence)
        for pattern in self._sarcasm_markers:
            if pattern.search(text):
                markers_found.append(f"explicit: {pattern.pattern}")
                confidence_signals.append(0.9)  # High confidence for explicit markers
        
        # Check hyperbolic patterns (medium-high confidence)
        for pattern in self._hyperbolic_patterns:
            if pattern.search(text):
                markers_found.append(f"hyperbolic: {pattern.pattern}")
                confidence_signals.append(0.75)
        
        # Check rhetorical questions (medium confidence)
        for pattern in self._rhetorical_questions:
            if pattern.search(text):
                markers_found.append(f"rhetorical: {pattern.pattern}")
                confidence_signals.append(0.7)
        
        # Check ironic contrast patterns (medium confidence)
        for pos_pattern, neg_patterns in self._ironic_contrasts:
            if pos_pattern.search(text):
                for neg_pattern in neg_patterns:
                    if neg_pattern.search(text):
                        markers_found.append(f"ironic_contrast: {pos_pattern.pattern} + {neg_pattern.pattern}")
                        confidence_signals.append(0.65)
                        break  # One match per positive word is enough
        
        # Check for quotation marks around positive words (sarcastic emphasis)
        quote_pattern = re.compile(r'["\'](\w+(?:\s+\w+)?)["\']', re.IGNORECASE)
        quoted_words = quote_pattern.findall(text)
        positive_quoted = ['great', 'wonderful', 'genius', 'brilliant', 'smart', 'perfect', 'amazing']
        for quoted in quoted_words:
            if quoted.lower() in positive_quoted:
                markers_found.append(f"quoted_positive: '{quoted}'")
                confidence_signals.append(0.7)
        
        # Calculate overall confidence
        if confidence_signals:
            # Use max confidence, boosted slightly by multiple signals
            base_confidence = max(confidence_signals)
            boost = min(0.15, 0.05 * (len(confidence_signals) - 1))
            overall_confidence = min(0.95, base_confidence + boost)
        else:
            overall_confidence = 0.0
        
        is_sarcastic = overall_confidence >= 0.5
        
        # Only invert sentiment if we're confident enough
        should_invert = overall_confidence >= 0.6
        
        return {
            'is_sarcastic': is_sarcastic,
            'confidence': overall_confidence,
            'markers_found': markers_found,
            'should_invert': should_invert
        }
    
    def invert_sentiment(self, score: float, sarcasm_confidence: float) -> float:
        """
        Invert sentiment score based on sarcasm detection.
        
        Uses weighted inversion based on confidence - higher confidence
        means more complete inversion.
        
        Args:
            score: Original sentiment score (-1 to 1)
            sarcasm_confidence: Confidence in sarcasm detection (0 to 1)
            
        Returns:
            Inverted sentiment score
        """
        # Scale inversion by confidence
        # At 0.6 confidence: partial inversion
        # At 0.9+ confidence: nearly complete inversion
        inversion_factor = min(1.0, sarcasm_confidence * 1.2)
        
        # Invert the score, weighted by confidence
        # score * (1 - 2*factor) inverts when factor=1
        inverted = score * (1 - 2 * inversion_factor)
        
        return max(-1.0, min(1.0, inverted))


class EnhancedVADERPreprocessor:
    """Advanced text preprocessor for VADER analysis."""
    
    def __init__(self, slang_processor: CommunitySlangProcessor):
        self.slang_processor = slang_processor
        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        self.mention_pattern = re.compile(r'@\w+')
        self.hashtag_pattern = re.compile(r'#(\w+)')
        self.ticker_pattern = re.compile(r'\$([A-Z]{1,5})\b')
        self.multiple_spaces = re.compile(r'\s+')
        self.repeated_chars = re.compile(r'(.)\1{3,}')
        self.bullish_indicators = set(FinancialLexicon.BULLISH_TERMS.keys())
        self.bearish_indicators = set(FinancialLexicon.BEARISH_TERMS.keys())
    
    def preprocess_with_metadata(self, text: str) -> Tuple[str, Dict[str, Any]]:
        """Enhanced preprocessing with metadata extraction."""
        if not text:
            return "", {}
        
        metadata = {
            'original_length': len(text),
            'word_count': len(text.split()),
        }
        
        # Extract tickers
        tickers = self.ticker_pattern.findall(text)
        metadata['tickers'] = tickers
        metadata['ticker_count'] = len(tickers)
        
        # Process community slang
        text = self.slang_processor.process_slang(text)
        metadata['community_slang'] = len([
            word for word in text.lower().split() 
            if word in self.slang_processor.SLANG_MAPPINGS
        ])
        
        # Extract emoji sentiment
        metadata['emoji_sentiment'] = self.slang_processor.extract_emoji_sentiment(text)
        
        # Remove URLs, mentions
        text = self.url_pattern.sub(' LINK ', text)
        text = self.mention_pattern.sub(' USER ', text)
        text = self.hashtag_pattern.sub(r'\1', text)
        text = self.repeated_chars.sub(r'\1\1', text)
        
        # Detect negative percentage patterns (e.g., "Down 40%")
        import re
        negative_percent_pattern = re.compile(r'\b(down|loss|losses|lost|drop|dropped|falling)\s+\d+%', re.IGNORECASE)
        if negative_percent_pattern.search(text):
            # Boost negative signal for detected negative percentage
            metadata['negative_indicators'] = metadata.get('negative_indicators', 0) + 2
            metadata['has_strong_signal'] = True
        
        # Count sentiment indicators
        text_lower = text.lower()
        metadata['positive_indicators'] = sum(1 for word in self.bullish_indicators if word in text_lower)
        metadata['negative_indicators'] = metadata.get('negative_indicators', 0) + sum(1 for word in self.bearish_indicators if word in text_lower)
        metadata['sentiment_indicators'] = metadata['positive_indicators'] + metadata['negative_indicators']
        
        metadata['has_strong_signal'] = (
            metadata['sentiment_indicators'] > 3 or
            abs(metadata['emoji_sentiment']) > 1.5 or
            any(word in text_lower for word in ['moon', 'crash', 'squeeze', 'dump'])
        )
        
        # Financial term count
        all_financial = self.bullish_indicators | self.bearish_indicators
        metadata['financial_terms'] = sum(1 for word in all_financial if word in text_lower)
        
        # Normalize whitespace
        text = self.multiple_spaces.sub(' ', text)
        text = text.strip()
        
        return text, metadata


class VADERModel(SentimentModel):
    """
    Enhanced VADER sentiment analysis model (internal use by Hybrid VADER).
    
    This is the rule-based component that provides:
    - Financial lexicon integration (75 terms)
    - Community slang and abbreviation processing (40+ phrases)
    - Emoji sentiment boosting (30+ emojis)
    - Sarcasm detection and sentiment inversion
    - Dynamic threshold adjustment
    - Context-aware analysis
    """
    
    def __init__(self, config: Optional[EnhancementConfig] = None):
        self.config = config or EnhancementConfig()
        self.analyzer = None
        self.financial_lexicon = FinancialLexicon()
        self.slang_processor = CommunitySlangProcessor()
        self.sarcasm_detector = SarcasmDetector()  # Sarcasm detection
        self._preprocessor = EnhancedVADERPreprocessor(self.slang_processor)
        
        self.custom_lexicon = {}
        self._build_custom_lexicon()
        
        super().__init__()
    
    def _build_custom_lexicon(self):
        """Build custom lexicon from financial terms."""
        if self.config.use_financial_lexicon:
            self.custom_lexicon.update(self.financial_lexicon.BULLISH_TERMS)
            self.custom_lexicon.update(self.financial_lexicon.BEARISH_TERMS)
            self.custom_lexicon.update(self.financial_lexicon.NEUTRAL_MODIFIERS)
    
    def _initialize_model_info(self) -> ModelInfo:
        """Initialize enhanced VADER model metadata."""
        return ModelInfo(
            name="VADER",
            version="1.0.0-enhanced",
            description="VADER with financial domain expertise",
            supported_sources=[DataSource.HACKERNEWS],
            max_batch_size=100,
            avg_processing_time=7.0
        )
    
    async def _load_model(self) -> None:
        """Load enhanced VADER analyzer with custom lexicon."""
        try:
            if not NLTK_AVAILABLE:
                raise ModelLoadError("NLTK and vaderSentiment not available")
            
            try:
                nltk.data.find('vader_lexicon')
            except LookupError:
                logger.info("Downloading VADER lexicon...")
                nltk.download('vader_lexicon', quiet=True)
            
            self.analyzer = VaderSIA()
            
            if self.custom_lexicon:
                self.analyzer.lexicon.update(self.custom_lexicon)
                logger.info(f"Added {len(self.custom_lexicon)} custom terms to VADER lexicon")
            
            test_result = self.analyzer.polarity_scores("test")
            if not isinstance(test_result, dict):
                raise ModelLoadError("VADER analyzer initialization failed")
            
            logger.info("Enhanced VADER model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load Enhanced VADER model: {str(e)}")
            raise ModelLoadError(f"Enhanced VADER model loading failed: {str(e)}")
    
    async def _analyze_batch(self, texts: List[str]) -> List[SentimentResult]:
        """Analyze sentiment with enhancements and sarcasm detection."""
        if not self.analyzer:
            raise AnalysisError("Enhanced VADER analyzer not loaded")
        
        results = []
        
        for text in texts:
            start_time = time.time()
            
            try:
                # Detect sarcasm BEFORE preprocessing (patterns need original text)
                sarcasm_result = self.sarcasm_detector.detect_sarcasm(text)
                
                processed_text, metadata = self._preprocessor.preprocess_with_metadata(text)
                
                # Add sarcasm info to metadata
                metadata['sarcasm'] = sarcasm_result
                
                emoji_boost = 0.0
                if self.config.use_emoji_boost:
                    emoji_boost = self.slang_processor.extract_emoji_sentiment(text)
                
                vader_scores = self.analyzer.polarity_scores(processed_text)
                
                if emoji_boost != 0:
                    vader_scores['compound'] = self._apply_boost(vader_scores['compound'], emoji_boost)
                
                if self.config.use_context_awareness:
                    vader_scores = self._apply_context_adjustments(vader_scores, metadata)
                
                # Apply sarcasm inversion if detected with high confidence
                if sarcasm_result['should_invert']:
                    original_compound = vader_scores['compound']
                    vader_scores['compound'] = self.sarcasm_detector.invert_sentiment(
                        vader_scores['compound'],
                        sarcasm_result['confidence']
                    )
                    # Also swap pos/neg scores
                    vader_scores['pos'], vader_scores['neg'] = vader_scores['neg'], vader_scores['pos']
                    logger.debug(
                        f"Sarcasm detected, inverted sentiment: {original_compound:.3f} -> {vader_scores['compound']:.3f}"
                    )
                
                result = self._convert_enhanced_result(vader_scores, metadata, time.time() - start_time)
                results.append(result)
                
            except Exception as e:
                logger.error(f"Enhanced VADER analysis failed: {str(e)}")
                results.append(self._create_error_result(time.time() - start_time, str(e)))
        
        return results
    
    def _apply_boost(self, base_score: float, boost: float) -> float:
        """Apply sentiment boost while keeping score in [-1, 1] range."""
        boosted = base_score + boost
        return max(-1.0, min(1.0, boosted))
    
    def _apply_context_adjustments(self, scores: Dict[str, float], metadata: Dict[str, Any]) -> Dict[str, float]:
        """Apply context-based score adjustments."""
        adjusted_scores = scores.copy()
        
        if metadata.get('has_strong_signal', False):
            factor = self.config.confidence_boost_factor
            if adjusted_scores['compound'] > 0:
                adjusted_scores['pos'] *= factor
            elif adjusted_scores['compound'] < 0:
                adjusted_scores['neg'] *= factor
        
        if metadata.get('sentiment_indicators', 0) > 2:
            adjusted_scores['neu'] *= 0.8
            
        total = adjusted_scores['pos'] + adjusted_scores['neu'] + adjusted_scores['neg']
        if total > 0:
            adjusted_scores['pos'] /= total
            adjusted_scores['neu'] /= total
            adjusted_scores['neg'] /= total
        
        return adjusted_scores
    
    def _convert_enhanced_result(self, vader_scores: Dict[str, float], metadata: Dict[str, Any], processing_time: float) -> SentimentResult:
        """Convert VADER scores to enhanced SentimentResult."""
        compound = vader_scores['compound']
        pos_score = vader_scores['pos']
        neu_score = vader_scores['neu']
        neg_score = vader_scores['neg']
        
        pos_threshold = self.config.positive_threshold
        neg_threshold = self.config.negative_threshold
        
        if self.config.use_dynamic_thresholds:
            if metadata.get('word_count', 0) < 10:
                pos_threshold *= 1.5
                neg_threshold *= 1.5
            elif metadata.get('word_count', 0) > 50:
                pos_threshold *= 0.8
                neg_threshold *= 0.8
        
        if compound >= pos_threshold:
            label = SentimentLabel.POSITIVE
            confidence = pos_score
            if metadata.get('positive_indicators', 0) > 0:
                confidence = min(1.0, confidence * 1.2)
        elif compound <= neg_threshold:
            label = SentimentLabel.NEGATIVE
            confidence = neg_score
            if metadata.get('negative_indicators', 0) > 0:
                confidence = min(1.0, confidence * 1.2)
        else:
            label = SentimentLabel.NEUTRAL
            confidence = neu_score
        
        # Extract sarcasm detection info
        sarcasm_info = metadata.get('sarcasm', {})
        
        return SentimentResult(
            label=label,
            score=compound,
            confidence=confidence,
            raw_scores={
                'compound': compound,
                'positive': pos_score,
                'neutral': neu_score,
                'negative': neg_score,
                'enhancements': {
                    'emoji_sentiment': metadata.get('emoji_sentiment', 0),
                    'financial_terms': metadata.get('financial_terms', 0),
                    'community_slang': metadata.get('community_slang', 0),
                    'sarcasm_detected': sarcasm_info.get('is_sarcastic', False),
                    'sarcasm_confidence': sarcasm_info.get('confidence', 0.0),
                    'sarcasm_inverted': sarcasm_info.get('should_invert', False),
                }
            },
            processing_time=processing_time * 1000,
            model_name=self.model_info.name
        )
    
    def _create_error_result(self, processing_time: float, error_msg: str) -> SentimentResult:
        """Create a neutral result when analysis fails."""
        return SentimentResult(
            label=SentimentLabel.NEUTRAL,
            score=0.0,
            confidence=0.0,
            raw_scores={'error': error_msg},
            processing_time=processing_time * 1000,
            model_name=self.model_info.name
        )


# =============================================================================
# Hybrid VADER Model (Main Export)
# =============================================================================


@dataclass
class HybridConfig:
    """Configuration for Hybrid VADER model."""
    vader_weight: float = 0.4  # Weight for VADER in ensemble
    ml_weight: float = 0.6  # Weight for ML in ensemble
    high_confidence_threshold: float = 0.7  # Threshold to trust VADER alone
    vader_high_confidence_weight: float = 0.8  # VADER weight when highly confident
    model_path: str = "data/models/hybrid_vader_lr.pkl"
    vectorizer_path: str = "data/models/hybrid_vader_vectorizer.pkl"
    max_features: int = 5000  # TF-IDF vocabulary size


class HybridVADERModel(SentimentModel):
    """
    Hybrid sentiment model combining Enhanced VADER with Logistic Regression.
    
    Features:
    - Enhanced VADER for rule-based sentiment with financial domain expertise
    - Logistic Regression for learning from labeled community data (HackerNews/forums)
    - Confidence-weighted ensemble fusion
    - Feature engineering: TF-IDF + VADER scores + metadata
    
    Performance:
    - Target: 82-87% accuracy on community financial sentiment
    - Improves over Enhanced VADER alone (73-78%)
    - Better handling of ambiguous cases and edge cases
    """
    
    def __init__(self, config: HybridConfig = None):
        """
        Initialize Hybrid VADER model.
        
        Args:
            config: Hybrid model configuration
        """
        super().__init__()
        self.config = config or HybridConfig()
        
        # Initialize Enhanced VADER with full configuration
        vader_config = EnhancementConfig(
            use_financial_lexicon=True,
            use_community_slang=True,
            use_emoji_boost=True,
            use_dynamic_thresholds=True,
            use_context_awareness=True
        )
        self.vader_model = VADERModel(vader_config)
        
        # ML components (loaded from disk if available)
        self.ml_model: Optional[LogisticRegression] = None
        self.vectorizer: Optional[TfidfVectorizer] = None
        self.scaler: Optional[StandardScaler] = None
        self.is_ml_trained = False
    
    async def _load_model(self) -> None:
        """Load both VADER and ML components."""
        logger.info("Loading Hybrid VADER model components...")
        
        # Load Enhanced VADER
        await self.vader_model._load_model()
        logger.info("Enhanced VADER component loaded")
        
        # Try to load trained ML model
        if not SKLEARN_AVAILABLE:
            logger.warning("scikit-learn not available - using Enhanced VADER only")
            return
        
        model_path = self.config.model_path
        vectorizer_path = self.config.vectorizer_path
        scaler_path = self.config.model_path.replace('_lr.pkl', '_scaler.pkl')
        
        if os.path.exists(model_path) and os.path.exists(vectorizer_path):
            try:
                with open(model_path, 'rb') as f:
                    self.ml_model = pickle.load(f)
                with open(vectorizer_path, 'rb') as f:
                    self.vectorizer = pickle.load(f)
                if os.path.exists(scaler_path):
                    with open(scaler_path, 'rb') as f:
                        self.scaler = pickle.load(f)
                self.is_ml_trained = True
                logger.info(f"Loaded pre-trained ML component from {model_path}")
            except Exception as e:
                logger.error(f"Failed to load ML component: {e}")
                logger.warning("Will use Enhanced VADER only")
        else:
            logger.info("ML component not found - will use Enhanced VADER only until trained")
            logger.info(f"Expected paths: {model_path}, {vectorizer_path}")
    
    async def _analyze_batch(self, texts: List[str]) -> List[SentimentResult]:
        """
        Hybrid analysis combining VADER and ML.
        
        Args:
            texts: List of text strings to analyze
            
        Returns:
            List of sentiment results with ensemble predictions
        """
        # Get Enhanced VADER results first
        vader_results = await self.vader_model._analyze_batch(texts)
        
        if not self.is_ml_trained:
            # ML not trained yet - use Enhanced VADER only
            # Update model name to indicate hybrid capability
            for result in vader_results:
                result.model_name = "Hybrid-VADER (VADER-only mode)"
            return vader_results
        
        # Get ML predictions
        ml_results = self._predict_ml(texts, vader_results)
        
        # Ensemble fusion
        final_results = []
        for text, vader_result, ml_result in zip(texts, vader_results, ml_results):
            final_result = self._ensemble_fusion(text, vader_result, ml_result)
            final_results.append(final_result)
        
        return final_results
    
    def _predict_ml(self, texts: List[str], vader_results: List[SentimentResult]) -> List[Dict]:
        """
        Get ML model predictions with feature engineering.
        
        Args:
            texts: Original text strings
            vader_results: Enhanced VADER results
            
        Returns:
            List of ML prediction dictionaries
        """
        # Feature extraction
        features = []
        for text, vader_result in zip(texts, vader_results):
            feature_vector = self._extract_features(text, vader_result)
            features.append(feature_vector)
        
        features_matrix = np.array(features)
        
        # Scale features if scaler is available
        if self.scaler is not None:
            features_matrix = self.scaler.transform(features_matrix)
        
        # Predict
        ml_probs = self.ml_model.predict_proba(features_matrix)
        ml_labels = self.ml_model.predict(features_matrix)
        
        return [
            {
                'label': self._int_to_label(label),
                'probs': probs,
                'confidence': max(probs),
                'class_index': label
            }
            for label, probs in zip(ml_labels, ml_probs)
        ]
    
    def _extract_features(self, text: str, vader_result: SentimentResult) -> np.ndarray:
        """
        Extract hybrid features combining TF-IDF and financial-specific features.
        
        Features (3012 dimensions):
        - TF-IDF vector (3000 dims with bigrams)
        - Financial features (12 dims)
        
        Must match the feature extraction used during training.
        
        Args:
            text: Original text
            vader_result: VADER analysis result (not used in new version)
            
        Returns:
            Feature vector as numpy array
        """
        # TF-IDF features (3000 dims)
        tfidf_features = self.vectorizer.transform([text]).toarray()[0]
        
        # Financial-specific features (12 dims) - must match training
        text_lower = text.lower()
        
        # Financial action words
        bullish_actions = ['buy', 'upgrade', 'outperform', 'overweight', 'accumulate', 
                           'bullish', 'long', 'calls', 'raise', 'ups', 'starts at buy']
        bearish_actions = ['sell', 'downgrade', 'underperform', 'underweight', 'reduce',
                           'bearish', 'short', 'puts', 'cut', 'cuts', 'lower', 'weakness']
        neutral_actions = ['hold', 'neutral', 'maintain', 'equal-weight', 'in-line']
        
        bullish_count = sum(1 for word in bullish_actions if word in text_lower)
        bearish_count = sum(1 for word in bearish_actions if word in text_lower)
        neutral_count = sum(1 for word in neutral_actions if word in text_lower)
        
        # Price movement indicators
        price_up = any(w in text_lower for w in ['surge', 'soar', 'jump', 'gain', 'rise', 'climb', 'rally'])
        price_down = any(w in text_lower for w in ['fall', 'drop', 'slide', 'plunge', 'decline', 'tank', 'crash'])
        
        # Analyst actions
        has_ticker = '$' in text
        has_upgrade = 'upgrade' in text_lower or 'ups' in text_lower or 'raise' in text_lower
        has_downgrade = 'downgrade' in text_lower or 'cut' in text_lower or 'lower' in text_lower
        
        # Rating words
        has_buy_rating = any(w in text_lower for w in ['buy', 'outperform', 'overweight'])
        has_sell_rating = any(w in text_lower for w in ['sell', 'underperform', 'underweight'])
        has_hold_rating = any(w in text_lower for w in ['hold', 'neutral', 'equal-weight'])
        
        financial_features = [
            bullish_count,
            bearish_count,
            neutral_count,
            int(price_up),
            int(price_down),
            int(has_ticker),
            int(has_upgrade),
            int(has_downgrade),
            int(has_buy_rating),
            int(has_sell_rating),
            int(has_hold_rating),
            bullish_count - bearish_count,  # sentiment_direction
        ]
        
        # Combine all features (3000 + 12 = 3012 dims)
        all_features = np.concatenate([
            tfidf_features,
            financial_features
        ])
        
        return all_features
    
    def _ensemble_fusion(self, text: str, vader_result: SentimentResult, 
                         ml_result: Dict) -> SentimentResult:
        """
        Fuse VADER and ML predictions using confidence weighting.
        
        Strategy:
        - If VADER confidence > 0.7: Use VADER primarily (80% weight)
        - If VADER detects strong negative (< -0.5): Trust VADER more (70% weight)
        - Else: Blend both models (60% ML, 40% VADER)
        
        Args:
            text: Original text
            vader_result: Enhanced VADER result
            ml_result: ML prediction dictionary
            
        Returns:
            Final ensemble sentiment result
        """
        vader_conf = vader_result.confidence
        vader_score = vader_result.score
        ml_conf = ml_result['confidence']
        
        # Check for strong negative sentiment from VADER
        # (ML model is biased toward positive, so trust VADER more for negatives)
        is_strong_negative = vader_score < -0.5
        has_negative_indicators = (
            vader_result.raw_scores.get('negative', 0) > 0.4 or
            vader_result.raw_scores.get('enhancements', {}).get('emoji_boost', 0) < -0.3
        )
        
        # Determine ensemble weights based on signals
        if vader_conf > self.config.high_confidence_threshold:
            # High confidence VADER - trust it more
            weight_vader = self.config.vader_high_confidence_weight
            weight_ml = 1.0 - weight_vader
            strategy = "vader_dominant"
        elif is_strong_negative and has_negative_indicators:
            # Strong negative signals from VADER - trust it more (ML is biased positive)
            weight_vader = 0.75
            weight_ml = 0.25
            strategy = "vader_negative_dominant"
        else:
            # Uncertain - blend both
            weight_vader = self.config.vader_weight
            weight_ml = self.config.ml_weight
            strategy = "balanced_blend"
        
        # Convert ML label to score for blending
        ml_score = self._label_to_score(ml_result['label'], ml_result['probs'])
        
        # Weighted score
        final_score = (
            weight_vader * vader_result.score + 
            weight_ml * ml_score
        )
        
        # Determine final label from blended score
        if final_score > 0.05:
            final_label = SentimentLabel.POSITIVE
        elif final_score < -0.05:
            final_label = SentimentLabel.NEGATIVE
        else:
            final_label = SentimentLabel.NEUTRAL
        
        # Weighted confidence
        final_confidence = weight_vader * vader_conf + weight_ml * ml_conf
        
        # Build comprehensive raw scores
        raw_scores = {
            'vader': vader_result.raw_scores,
            'ml': {
                'label': ml_result['label'].value,
                'probs': ml_result['probs'].tolist(),
                'confidence': ml_conf
            },
            'ensemble': {
                'strategy': strategy,
                'weights': {
                    'vader': weight_vader,
                    'ml': weight_ml
                },
                'vader_score': vader_result.score,
                'ml_score': ml_score,
                'final_score': final_score
            }
        }
        
        return SentimentResult(
            label=final_label,
            score=final_score,
            confidence=final_confidence,
            raw_scores=raw_scores,
            processing_time=vader_result.processing_time,
            model_name="Hybrid-VADER"
        )
    
    def _label_to_score(self, label: SentimentLabel, probs: np.ndarray) -> float:
        """
        Convert ML label and probabilities to a continuous score (-1 to 1).
        
        Args:
            label: Predicted sentiment label
            probs: Class probabilities [neg, neu, pos]
            
        Returns:
            Score in range [-1, 1]
        """
        # Extract class probabilities (assuming order: neg=0, neu=1, pos=2)
        if len(probs) >= 3:
            neg_prob = probs[0]
            neu_prob = probs[1]
            pos_prob = probs[2]
        else:
            # Fallback if unexpected probability shape
            if label == SentimentLabel.POSITIVE:
                return 0.5
            elif label == SentimentLabel.NEGATIVE:
                return -0.5
            else:
                return 0.0
        
        # Calculate weighted score
        score = pos_prob * 1.0 + neu_prob * 0.0 + neg_prob * (-1.0)
        
        return score
    
    def _int_to_label(self, label_int: int) -> SentimentLabel:
        """Convert integer label to SentimentLabel enum."""
        label_map = {
            0: SentimentLabel.NEGATIVE,
            1: SentimentLabel.NEUTRAL,
            2: SentimentLabel.POSITIVE
        }
        return label_map.get(label_int, SentimentLabel.NEUTRAL)
    
    async def train_ml_component(self, X_train: List[str], y_train: List[int],
                                 X_val: List[str] = None, y_val: List[int] = None):
        """
        Train the ML component on labeled community data.
        
        Args:
            X_train: List of comment texts (HackerNews/forum posts)
            y_train: Labels (0=negative, 1=neutral, 2=positive)
            X_val: Optional validation texts
            y_val: Optional validation labels
        """
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learn is required for training. Install with: pip install scikit-learn")
        
        logger.info(f"Training Hybrid VADER ML component on {len(X_train)} samples...")
        
        # Initialize TF-IDF vectorizer
        self.vectorizer = TfidfVectorizerImpl(
            max_features=self.config.max_features,
            ngram_range=(1, 2),  # Unigrams and bigrams
            min_df=2,  # Minimum document frequency
            max_df=0.95,  # Maximum document frequency
            strip_accents='unicode',
            lowercase=True,
            stop_words='english'
        )
        
        # Fit TF-IDF on training data
        logger.info("Fitting TF-IDF vectorizer...")
        self.vectorizer.fit(X_train)
        
        # Get VADER results for training data
        logger.info("Getting Enhanced VADER results for training data...")
        vader_results = await self.vader_model._analyze_batch(X_train)
        
        # Extract features
        logger.info("Extracting features...")
        features = []
        for text, vader_result in zip(X_train, vader_results):
            feature_vector = self._extract_features(text, vader_result)
            features.append(feature_vector)
        features_matrix = np.array(features)
        
        logger.info(f"Feature matrix shape: {features_matrix.shape}")
        
        # Scale features for better convergence
        logger.info("Scaling features...")
        self.scaler = StandardScalerImpl()
        features_scaled = self.scaler.fit_transform(features_matrix)
        
        # Train Logistic Regression
        logger.info("Training Logistic Regression classifier...")
        self.ml_model = LogisticRegressionImpl(
            max_iter=1000,
            class_weight='balanced',  # Handle class imbalance
            random_state=42,
            solver='lbfgs',  # Good for multiclass, works well with scaling
            C=1.0  # Regularization strength
        )
        self.ml_model.fit(features_scaled, y_train)
        
        # Evaluate on training data
        train_accuracy = self.ml_model.score(features_scaled, y_train)
        logger.info(f"Training accuracy: {train_accuracy:.4f}")
        
        # Evaluate on validation data if provided
        if X_val and y_val:
            logger.info("Evaluating on validation data...")
            val_vader_results = await self.vader_model._analyze_batch(X_val)
            val_features = []
            for text, vader_result in zip(X_val, val_vader_results):
                feature_vector = self._extract_features(text, vader_result)
                val_features.append(feature_vector)
            val_features_matrix = np.array(val_features)
            val_features_scaled = self.scaler.transform(val_features_matrix)
            
            val_accuracy = self.ml_model.score(val_features_scaled, y_val)
            logger.info(f"Validation accuracy: {val_accuracy:.4f}")
        else:
            val_accuracy = None
        
        # Save model and scaler
        os.makedirs(os.path.dirname(self.config.model_path), exist_ok=True)
        
        with open(self.config.model_path, 'wb') as f:
            pickle.dump(self.ml_model, f)
        logger.info(f"ML model saved to {self.config.model_path}")
        
        with open(self.config.vectorizer_path, 'wb') as f:
            pickle.dump(self.vectorizer, f)
        logger.info(f"Vectorizer saved to {self.config.vectorizer_path}")
        
        scaler_path = self.config.model_path.replace('_lr.pkl', '_scaler.pkl')
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
        logger.info(f"Scaler saved to {scaler_path}")
        
        self.is_ml_trained = True
        logger.info("ML component trained and saved successfully")
        
        return {
            'train_accuracy': train_accuracy,
            'val_accuracy': val_accuracy if X_val else None,
            'n_features': features_matrix.shape[1],
            'n_samples': len(X_train)
        }
    
    def _initialize_model_info(self) -> ModelInfo:
        """Initialize model information (required by base class)."""
        return ModelInfo(
            name="Hybrid-VADER",
            version="1.0.0",
            description="Enhanced VADER + Logistic Regression ensemble for community sentiment",
            supported_sources=[DataSource.HACKERNEWS],
            max_batch_size=32,
            avg_processing_time=15.0  # ms per text
        )
    
    def get_model_info(self) -> ModelInfo:
        """Get model information."""
        info = self.model_info
        info.metadata = {
            'vader_enhanced': True,
            'ml_trained': self.is_ml_trained,
            'ml_available': SKLEARN_AVAILABLE,
            'ensemble_strategy': 'confidence_weighted',
            'vader_weight': self.config.vader_weight,
            'ml_weight': self.config.ml_weight
        }
        return info
